import streamlit as st
import os
import time
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

# ==========================================
# 1. ì„¤ì • ë° API í‚¤
# ==========================================

# ë°ì´í„° ê²½ë¡œ (ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)
DATA_DIR = "C:/RAG_DATA/data/test"

# OpenRouter / OpenAI API í‚¤ ì„¤ì •
# ì‹¤ì œ í‚¤ë¥¼ ì—¬ê¸°ì— ì…ë ¥í•˜ê±°ë‚˜ í™˜ê²½ ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ì„¸ìš”.
# os.environ["OPENROUTER_API_KEY"] = "sk-or-v1-..." 
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY", YOUR_API_KEY)

OPENROUTER_API_BASE = "https://openrouter.ai/api/v1"

LLM_MODEL_NAME = "google/gemini-2.5-flash-preview-09-2025"
EMBEDDING_MODEL_NAME = "openai/text-embedding-3-small"

# ==========================================
# 2. í•¨ìˆ˜ ì •ì˜ (ëª¨ë¸ ë¡œë“œ ë° ë‹µë³€ ìƒì„±)
# ==========================================

@st.cache_resource
def load_faiss_index(folder_path, index_name):
    """
    FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. (ìºì‹± ì ìš©ìœ¼ë¡œ ì†ë„ í–¥ìƒ)
    """
    # [Fix] OpenRouter ì‚¬ìš© ì‹œ openai_api_baseì™€ api_keyë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.
    embeddings = OpenAIEmbeddings(
        model=EMBEDDING_MODEL_NAME,
        openai_api_key=OPENROUTER_API_KEY,
        openai_api_base=OPENROUTER_API_BASE,
        check_embedding_ctx_length=False
    )
    try:
        vectorstore = FAISS.load_local(
            folder_path=folder_path, 
            embeddings=embeddings, 
            index_name=index_name,
            allow_dangerous_deserialization=True 
        )
        return vectorstore
    except Exception as e:
        st.error(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def generate_response(llm, context, question):
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    prompt = f"""You are a helpful and rigorous AI assistant grounded in Database Systems knowledge.
Answer the question based ONLY on the following context.
If the answer is not in the context, say "I don't know based on the provided context."

Context:
{context}

Question: {question}

Answer:"""
    
    messages = [
        SystemMessage(content="You are a helpful assistant."),
        HumanMessage(content=prompt)
    ]
    
    response = llm.invoke(messages)
    return response.content.strip()

# ==========================================
# 3. Streamlit UI êµ¬ì„±
# ==========================================

st.set_page_config(page_title="RAPTOR RAG Demo", page_icon="ğŸ¦–", layout="wide")

st.title("ğŸ¦– RAPTOR RAG Q&A System")
st.markdown("ë°ì´í„°ë² ì´ìŠ¤ ì „ê³µ ì„œì  ì§€ì‹ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ **ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ**ì…ë‹ˆë‹¤.")

# --- ì‚¬ì´ë“œë°”: ëª¨ë¸ ì„ íƒ ---
st.sidebar.header("âš™ï¸ ì„¤ì • (Settings)")

# ëª¨ë¸ í´ë” ìŠ¤ìº”
if os.path.exists(DATA_DIR):
    # .faiss íŒŒì¼ì´ ìˆëŠ” í´ë”ë§Œ ì°¾ê¸° (ì¬ê·€ì  íƒìƒ‰)
    model_options = []
    for root, dirs, files in os.walk(DATA_DIR):
        for file in files:
            if file.endswith(".faiss"):
                # ê²½ë¡œì—ì„œ ëª¨ë¸ ì´ë¦„ ì¶”ì¶œ (íŒŒì¼ëª… ë˜ëŠ” í´ë”ëª…)
                model_name = os.path.splitext(file)[0]
                full_path = root
                model_options.append((model_name, full_path))
    
    if not model_options:
        st.sidebar.warning(f"No .faiss files found in {DATA_DIR}")
        st.stop()

    # ì„ íƒ ë°•ìŠ¤ (í‘œì‹œ ì´ë¦„: ëª¨ë¸ëª…)
    selected_option = st.sidebar.selectbox(
        "ì‚¬ìš©í•  ëª¨ë¸ ì„ íƒ", 
        options=model_options, 
        format_func=lambda x: x[0] # ëª¨ë¸ëª…ë§Œ í‘œì‹œ
    )
    
    selected_model_name, selected_model_path = selected_option
    
    # ê²€ìƒ‰ ì„¤ì •
    top_k = st.sidebar.slider("ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜ (Top-K)", min_value=1, max_value=10, value=5)

else:
    st.sidebar.error(f"ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {DATA_DIR}")
    st.stop()

# --- ëª¨ë¸ ë¡œë“œ ---
if selected_model_name:
    vectorstore = load_faiss_index(selected_model_path, selected_model_name)
    if vectorstore:
        st.sidebar.success(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {selected_model_name}")
    else:
        st.stop()

# --- ë©”ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ---

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ì±„íŒ… ê¸°ë¡ ì €ì¥)
if "messages" not in st.session_state:
    st.session_state.messages = []

# ê¸°ì¡´ ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        # ë§Œì•½ ì´ì „ì— ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ìˆë‹¤ë©´ í‘œì‹œ (assistant ë©”ì‹œì§€ì¸ ê²½ìš°)
        if "docs" in message:
            with st.expander("ğŸ“š ì°¸ê³ í•œ ë¬¸ì„œ (Retrieved Context) í™•ì¸í•˜ê¸°"):
                for i, doc in enumerate(message["docs"]):
                    st.markdown(f"**[Document {i+1}]**")
                    st.text(doc.page_content)
                    st.divider()

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”... (ì˜ˆ: What is ACID property?)"):
    # 1. ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ ë° ì €ì¥
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # 2. ë‹µë³€ ìƒì„± ê³¼ì •
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        
        # (1) ê²€ìƒ‰ (Retrieval)
        with st.status("ğŸ” ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤...", expanded=True) as status:
            try:
                retriever = vectorstore.as_retriever(search_kwargs={"k": top_k})
                retrieved_docs = retriever.invoke(prompt)
                status.update(label="ê²€ìƒ‰ ì™„ë£Œ!", state="complete", expanded=False)
                
                # ê²€ìƒ‰ëœ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸° (Expander)
                with st.expander("ğŸ“š ì°¸ê³ í•œ ë¬¸ì„œ (Retrieved Context) í™•ì¸í•˜ê¸°"):
                    context_text = ""
                    for i, doc in enumerate(retrieved_docs):
                        st.markdown(f"**[Document {i+1}]**")
                        st.text(doc.page_content)
                        st.divider()
                        context_text += doc.page_content + "\n\n"
            except Exception as e:
                status.update(label="ê²€ìƒ‰ ì‹¤íŒ¨", state="error", expanded=True)
                st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                st.stop()

        # (2) ìƒì„± (Generation)
        message_placeholder.markdown("ğŸ¤– ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...")
        
        try:
            llm = ChatOpenAI(
                model=LLM_MODEL_NAME,
                openai_api_key=OPENROUTER_API_KEY,
                openai_api_base=OPENROUTER_API_BASE,
                temperature=0
            )
            
            answer = generate_response(llm, context_text, prompt)
            
            # (3) ê²°ê³¼ í‘œì‹œ
            message_placeholder.markdown(answer)
            
            # ì„¸ì…˜ì— ì €ì¥ (ë¬¸ì„œ ì •ë³´ í¬í•¨)
            st.session_state.messages.append({
                "role": "assistant", 
                "content": answer,
                "docs": retrieved_docs
            })
        except Exception as e:
            st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")