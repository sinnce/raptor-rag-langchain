{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ae0a5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m314 packages\u001b[0m \u001b[2min 13ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m193 packages\u001b[0m \u001b[2min 606ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add langchain langchain-core langchain-community langchain-text-splitters langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7495b701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m314 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m193 packages\u001b[0m \u001b[2min 16ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add pypdf beautifulsoup4 selenium requests lxml jq tiktoken sentence-transformers langchain-huggingface faiss-cpu chromadb \"langchain[google-genai]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18e0c1b",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6d86f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Doyoon\\Desktop\\Doyoon\\2025 Fall\\BKMS1\\Assignment\\Assignment 3\\raptor-rag-langchain\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2195 pages (Documents).\n",
      "\n",
      "--- Page 1000 Content (Partial) ---\n",
      "Page 652\n",
      "When we use a B+-tree for ﬁle organization, space\n",
      "utilization is particularly important, since the space\n",
      "occupied by the records is likely to be much more than the\n",
      "space occupied by keys and pointers. We can improve the\n",
      "utilization of space in a B+-tree by involving more sibling\n",
      "nodes in redistribution during splits and merges. The\n",
      "technique is applicable to both leaf nodes and nonleaf\n",
      "nodes, and it works as follows:\n",
      "During insertion, if a node is full, the system attempts to\n",
      "redistribu\n",
      "\n",
      "--- Page 1000 Metadata ---\n",
      "{'producer': 'calibre 6.28.1', 'creator': 'calibre 6.28.1', 'creationdate': '2024-02-19T20:36:37+00:00', 'author': 'SILBERSCHATZ;', 'moddate': '2024-02-19T20:36:37+00:00', 'title': 'ISE EBook Online for Database System Concepts', 'source': '../data/raw/Database System Concepts 7th Ed.pdf', 'total_pages': 2195, 'page': 999, 'page_label': '1000'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# 1. Create the loader\n",
    "loader = PyPDFLoader(\"../data/raw/Database System Concepts 7th Ed.pdf\")\n",
    "\n",
    "# 2. Load and split (by page)\n",
    "pages = loader.load() # .load() returns a list of Documents\n",
    "\n",
    "# 3. Check the results\n",
    "print(f\"Loaded {len(pages)} pages (Documents).\")\n",
    "\n",
    "# Preview the content of page 0 (the first page)\n",
    "print(f\"\\n--- Page 1000 Content (Partial) ---\")\n",
    "print(pages[999].page_content[:500])\n",
    "\n",
    "# Metadata for page 0 (source and page number)\n",
    "# Notice how the metadata is automatically populated!\n",
    "print(f\"\\n--- Page 1000 Metadata ---\")\n",
    "print(pages[999].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54f63f5",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf12ccd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Split into 19706 chunks ---\n",
      "\n",
      "--- Chunk 1 ---\n",
      "D A T A B A S E \n",
      "SYSTEM CONCEPTS\n",
      "SEVENTH EDITION\n",
      "Abraham Silberschatz\n",
      "Yale University\n",
      "Henry F. Korth\n",
      "Lehigh University\n",
      "S. Sudarshan\n",
      "Indian Institute of Technology, Bombay\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Page ii\n",
      " \n",
      "DATABASE SYSTEM CONCEPTS\n",
      "Published by McGraw-Hill Education, 2 Penn Plaza, New York,\n",
      "NY 10121. Copyright © 2020 by McGraw-Hill Education. All\n",
      "\n",
      "--- Chunk 3 ---\n",
      "rights reserved. Printed in the United States of America. No\n",
      "part of this publication may be reproduced or distributed in\n",
      "any form or by any means, or stored in a database or\n",
      "\n",
      "--- Chunk 4 ---\n",
      "retrieval system, without the prior written consent of\n",
      "McGraw-Hill Education, including, but not limited to, in any\n",
      "network or other electronic storage or transmission, or\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1. Create the splitter (200 chars, 50 char overlap)\n",
    "# It will automatically use the separators [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# 2. Split the documents\n",
    "recursive_chunks = recursive_splitter.split_documents(pages)\n",
    "\n",
    "print(f\"--- Split into {len(recursive_chunks)} chunks ---\")\n",
    "\n",
    "# 3. Check the results (compare them to the CharacterTextSplitter)\n",
    "print(\"\\n--- Chunk 1 ---\")\n",
    "print(recursive_chunks[0].page_content)\n",
    "\n",
    "print(\"\\n--- Chunk 2 ---\")\n",
    "print(recursive_chunks[1].page_content)\n",
    "\n",
    "print(\"\\n--- Chunk 3 ---\")\n",
    "print(recursive_chunks[2].page_content)\n",
    "\n",
    "print(\"\\n--- Chunk 4 ---\")\n",
    "print(recursive_chunks[3].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b40f820",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "444857d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# the parameters for this model can be chosen from https://sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html#sentence_transformers.SentenceTransformer\n",
    "\n",
    "# If the backend supports cuda, we use it\n",
    "if torch.cuda.is_available():\n",
    "  model_kwargs = {\"device\": \"cuda\"}\n",
    "else:\n",
    "  model_kwargs = {\"device\": \"cpu\"}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"distiluse-base-multilingual-cased-v1\",\n",
    "    model_kwargs=model_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5e7df7",
   "metadata": {},
   "source": [
    "# Vector Store & Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7f323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# This will take a few minutes without GPU\n",
    "# However, you can finish it in just a few seconds with GPU!\n",
    "myvectorstore = FAISS.from_documents(recursive_chunks, embeddings)\n",
    "\n",
    "# You can store the index\n",
    "myvectorstore.save_local(\"myfaissidx\")\n",
    "# You can see the file now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93201209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='6d12f061-162b-405d-a9f8-3d4454741072', metadata={'producer': 'calibre 6.28.1', 'creator': 'calibre 6.28.1', 'creationdate': '2024-02-19T20:36:37+00:00', 'author': 'SILBERSCHATZ;', 'moddate': '2024-02-19T20:36:37+00:00', 'title': 'ISE EBook Online for Database System Concepts', 'source': '../data/raw/Database System Concepts 7th Ed.pdf', 'total_pages': 2195, 'page': 1830, 'page_label': '1831'}, page_content='dynamic hashing techniques.\\n24.1 Bloom Filter'), Document(id='60ce35ba-1c56-475b-9186-3b7f9f47632e', metadata={'producer': 'calibre 6.28.1', 'creator': 'calibre 6.28.1', 'creationdate': '2024-02-19T20:36:37+00:00', 'author': 'SILBERSCHATZ;', 'moddate': '2024-02-19T20:36:37+00:00', 'title': 'ISE EBook Online for Database System Concepts', 'source': '../data/raw/Database System Concepts 7th Ed.pdf', 'total_pages': 2195, 'page': 1088, 'page_label': '1089'}, page_content='ﬁgure.'), Document(id='6acb542b-c39b-4a27-873a-6df5a0d08c2f', metadata={'producer': 'calibre 6.28.1', 'creator': 'calibre 6.28.1', 'creationdate': '2024-02-19T20:36:37+00:00', 'author': 'SILBERSCHATZ;', 'moddate': '2024-02-19T20:36:37+00:00', 'title': 'ISE EBook Online for Database System Concepts', 'source': '../data/raw/Database System Concepts 7th Ed.pdf', 'total_pages': 2195, 'page': 1840, 'page_label': '1841'}, page_content='24.2.4.2 Lookup Operations Using Bloom Filters\\nLookup operations in stepped-merge index have to\\nseparately search each of the trees. Thus, compared to the'), Document(id='10162e69-0e88-4b6c-b74e-42223a596650', metadata={'producer': 'calibre 6.28.1', 'creator': 'calibre 6.28.1', 'creationdate': '2024-02-19T20:36:37+00:00', 'author': 'SILBERSCHATZ;', 'moddate': '2024-02-19T20:36:37+00:00', 'title': 'ISE EBook Online for Database System Concepts', 'source': '../data/raw/Database System Concepts 7th Ed.pdf', 'total_pages': 2195, 'page': 1967, 'page_label': '1968'}, page_content='to blockchains.4'), Document(id='4cc35d03-e530-40bb-a697-730f9d0bad8d', metadata={'producer': 'calibre 6.28.1', 'creator': 'calibre 6.28.1', 'creationdate': '2024-02-19T20:36:37+00:00', 'author': 'SILBERSCHATZ;', 'moddate': '2024-02-19T20:36:37+00:00', 'title': 'ISE EBook Online for Database System Concepts', 'source': '../data/raw/Database System Concepts 7th Ed.pdf', 'total_pages': 2195, 'page': 1841, 'page_label': '1842'}, page_content='only slightly worse than on a regular B+-tree.\\nThe Bloom ﬁlter check thus works very well for point\\nlookups, allowing a signiﬁcant fraction of the trees to be'), Document(id='5b926975-ca61-45e4-891a-f6a9163f4805', metadata={'producer': 'calibre 6.28.1', 'creator': 'calibre 6.28.1', 'creationdate': '2024-02-19T20:36:37+00:00', 'author': 'SILBERSCHATZ;', 'moddate': '2024-02-19T20:36:37+00:00', 'title': 'ISE EBook Online for Database System Concepts', 'source': '../data/raw/Database System Concepts 7th Ed.pdf', 'total_pages': 2195, 'page': 1840, 'page_label': '1841'}, page_content='and the system load is light, trees across all levels could\\npotentially get merged into a single tree at some level r.\\n24.2.4.2 Lookup Operations Using Bloom Filters'), Document(id='3a17c5cc-ddbe-4e29-8734-185fd4248481', metadata={'producer': 'calibre 6.28.1', 'creator': 'calibre 6.28.1', 'creationdate': '2024-02-19T20:36:37+00:00', 'author': 'SILBERSCHATZ;', 'moddate': '2024-02-19T20:36:37+00:00', 'title': 'ISE EBook Online for Database System Concepts', 'source': '../data/raw/Database System Concepts 7th Ed.pdf', 'total_pages': 2195, 'page': 18, 'page_label': '19'}, page_content='TOPICS\\nChapter 24 Advanced Indexing Techniques\\n24.1 Bloom Filter 1175\\n24.2 Log-Structured Merge Tree and Variants 1176'), Document(id='4adf791e-84ea-4b5f-bfa5-8a36e092cbdf', metadata={'producer': 'calibre 6.28.1', 'creator': 'calibre 6.28.1', 'creationdate': '2024-02-19T20:36:37+00:00', 'author': 'SILBERSCHATZ;', 'moddate': '2024-02-19T20:36:37+00:00', 'title': 'ISE EBook Online for Database System Concepts', 'source': '../data/raw/Database System Concepts 7th Ed.pdf', 'total_pages': 2195, 'page': 1841, 'page_label': '1842'}, page_content='skipped, as long as suﬃcient memory is available to store\\nall the Bloom ﬁlters in memory. With I key values in the\\nindex, approximately 10I bits of memory will be required. To'), Document(id='018ae65a-d5c8-4b94-a416-2c59f8b7bc6d', metadata={'producer': 'calibre 6.28.1', 'creator': 'calibre 6.28.1', 'creationdate': '2024-02-19T20:36:37+00:00', 'author': 'SILBERSCHATZ;', 'moddate': '2024-02-19T20:36:37+00:00', 'title': 'ISE EBook Online for Database System Concepts', 'source': '../data/raw/Database System Concepts 7th Ed.pdf', 'total_pages': 2195, 'page': 1841, 'page_label': '1842'}, page_content='reduce the main memory overhead, some of the Bloom\\nﬁlters may be stored on ﬂash storage.\\nNote that for range lookups, the Bloom ﬁlter optimization\\ncannot be used, since there is no unique hash value.'), Document(id='5a3e9de9-4f41-4d5f-8861-28456bf13e06', metadata={'producer': 'calibre 6.28.1', 'creator': 'calibre 6.28.1', 'creationdate': '2024-02-19T20:36:37+00:00', 'author': 'SILBERSCHATZ;', 'moddate': '2024-02-19T20:36:37+00:00', 'title': 'ISE EBook Online for Database System Concepts', 'source': '../data/raw/Database System Concepts 7th Ed.pdf', 'total_pages': 2195, 'page': 1831, 'page_label': '1832'}, page_content='elements that are in the set. A Bloom ﬁlter is basically a\\nbitmap. If the set has n values, the associated bitmap has a\\nfew times n (typically 10n) bits; the Bloom ﬁlter also has')]\n"
     ]
    }
   ],
   "source": [
    "# You can load the stored index.\n",
    "# You need to specify the embedding model by passing an embedding object parameter\n",
    "new_vector_store = FAISS.load_local(\n",
    "    \"myfaissidx\", embeddings, allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# we're doing it without any parameter, but you can set `k` and `filter`, etc.\n",
    "docs = new_vector_store.similarity_search(\"Bloom filter\", 10)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d135664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dynamic hashing techniques.\n",
      "24.1 Bloom Filter\n",
      "ﬁgure.\n",
      "24.2.4.2 Lookup Operations Using Bloom Filters\n",
      "Lookup operations in stepped-merge index have to\n",
      "separately search each of the trees. Thus, compared to the\n",
      "to blockchains.4\n",
      "only slightly worse than on a regular B+-tree.\n",
      "The Bloom ﬁlter check thus works very well for point\n",
      "lookups, allowing a signiﬁcant fraction of the trees to be\n",
      "and the system load is light, trees across all levels could\n",
      "potentially get merged into a single tree at some level r.\n",
      "24.2.4.2 Lookup Operations Using Bloom Filters\n",
      "TOPICS\n",
      "Chapter 24 Advanced Indexing Techniques\n",
      "24.1 Bloom Filter 1175\n",
      "24.2 Log-Structured Merge Tree and Variants 1176\n",
      "skipped, as long as suﬃcient memory is available to store\n",
      "all the Bloom ﬁlters in memory. With I key values in the\n",
      "index, approximately 10I bits of memory will be required. To\n",
      "reduce the main memory overhead, some of the Bloom\n",
      "ﬁlters may be stored on ﬂash storage.\n",
      "Note that for range lookups, the Bloom ﬁlter optimization\n",
      "cannot be used, since there is no unique hash value.\n",
      "elements that are in the set. A Bloom ﬁlter is basically a\n",
      "bitmap. If the set has n values, the associated bitmap has a\n",
      "few times n (typically 10n) bits; the Bloom ﬁlter also has\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(docs[i].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fa5d2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "myretriever = myvectorstore.as_retriever(search_type=\"mmr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd2e586c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='6d12f061-162b-405d-a9f8-3d4454741072', metadata={'producer': 'calibre 6.28.1', 'creator': 'calibre 6.28.1', 'creationdate': '2024-02-19T20:36:37+00:00', 'author': 'SILBERSCHATZ;', 'moddate': '2024-02-19T20:36:37+00:00', 'title': 'ISE EBook Online for Database System Concepts', 'source': '../data/raw/Database System Concepts 7th Ed.pdf', 'total_pages': 2195, 'page': 1830, 'page_label': '1831'}, page_content='dynamic hashing techniques.\\n24.1 Bloom Filter'),\n",
       " Document(id='4cc35d03-e530-40bb-a697-730f9d0bad8d', metadata={'producer': 'calibre 6.28.1', 'creator': 'calibre 6.28.1', 'creationdate': '2024-02-19T20:36:37+00:00', 'author': 'SILBERSCHATZ;', 'moddate': '2024-02-19T20:36:37+00:00', 'title': 'ISE EBook Online for Database System Concepts', 'source': '../data/raw/Database System Concepts 7th Ed.pdf', 'total_pages': 2195, 'page': 1841, 'page_label': '1842'}, page_content='only slightly worse than on a regular B+-tree.\\nThe Bloom ﬁlter check thus works very well for point\\nlookups, allowing a signiﬁcant fraction of the trees to be'),\n",
       " Document(id='60ce35ba-1c56-475b-9186-3b7f9f47632e', metadata={'producer': 'calibre 6.28.1', 'creator': 'calibre 6.28.1', 'creationdate': '2024-02-19T20:36:37+00:00', 'author': 'SILBERSCHATZ;', 'moddate': '2024-02-19T20:36:37+00:00', 'title': 'ISE EBook Online for Database System Concepts', 'source': '../data/raw/Database System Concepts 7th Ed.pdf', 'total_pages': 2195, 'page': 1088, 'page_label': '1089'}, page_content='ﬁgure.'),\n",
       " Document(id='10162e69-0e88-4b6c-b74e-42223a596650', metadata={'producer': 'calibre 6.28.1', 'creator': 'calibre 6.28.1', 'creationdate': '2024-02-19T20:36:37+00:00', 'author': 'SILBERSCHATZ;', 'moddate': '2024-02-19T20:36:37+00:00', 'title': 'ISE EBook Online for Database System Concepts', 'source': '../data/raw/Database System Concepts 7th Ed.pdf', 'total_pages': 2195, 'page': 1967, 'page_label': '1968'}, page_content='to blockchains.4')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myretriever.invoke(\"Bloom filter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86bbfaf",
   "metadata": {},
   "source": [
    "# LLM Integration & Building a RAG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3fc550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR API KEY\" # enter your api key\n",
    "\n",
    "model = init_chat_model(\"google_genai:gemini-2.5-flash\") # you may change to another model if you wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffc9d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "@dynamic_prompt\n",
    "def prompt_with_context(request: ModelRequest) -> str:\n",
    "    \"\"\"Inject context into state messages.\"\"\"\n",
    "    last_query = request.state[\"messages\"][-1].text\n",
    "\n",
    "    retrieved_docs = myvectorstore.similarity_search(last_query) # using the faiss vector store from our own dataset\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    system_message = (\n",
    "        \"You are a helpful assistant. Use the following context in your response:\"\n",
    "        f\"\\n\\n{docs_content}\"\n",
    "    )\n",
    "\n",
    "    return system_message\n",
    "\n",
    "\n",
    "agent = create_agent(model, tools=[], middleware=[prompt_with_context]) # using the Google Gemini model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5e2e647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Explain about Bloom filter in detail.', additional_kwargs={}, response_metadata={}, id='fefd74d1-f3b9-48b1-bfd0-fc1eae34c88a'),\n",
       "  AIMessage(content='A Bloom filter is a **space-efficient probabilistic data structure** that is used to test whether an element is a member of a set. It\\'s a clever way to check for set membership without storing the actual elements, allowing for significant memory savings, especially for very large sets.\\n\\nIt was invented by Burton Howard Bloom in 1970.\\n\\nHere\\'s a detailed breakdown:\\n\\n### Core Concept\\n\\nImagine you have a very large list of items (e.g., all the URLs you\\'ve ever visited, or all the potentially malicious IP addresses). You want to quickly check if a *new* item is in that list without storing the entire list in memory, which could be massive. A Bloom filter can tell you:\\n1.  **Definitely not in the set.**\\n2.  **Probably in the set** (with a small chance of error, called a \"false positive\").\\n\\n### How it Works\\n\\nA Bloom filter consists of two main components:\\n\\n1.  **A Bit Array (or Bitmap):** This is an array of `m` bits, all initially set to `0`. (As mentioned in the context: \"a Bloom ﬁlter with a bitmap b of size m, initialized with...\")\\n2.  **`k` Independent Hash Functions:** These are different hash functions that map an input element to a position within the bit array. They should ideally be fast and produce uniformly distributed outputs.\\n\\nLet\\'s illustrate the operations:\\n\\n#### 1. Adding an Element to the Set\\n\\nTo add an element `x` to the Bloom filter:\\n*   Apply each of the `k` hash functions to `x`.\\n*   Each hash function will produce an index (a number between `0` and `m-1`).\\n*   Set the bits at all `k` computed indices in the bit array to `1`.\\n*   If a bit is already `1`, it simply remains `1`.\\n\\n**Example:**\\nSuppose `m = 10` (bit array size) and `k = 3` (hash functions `h1`, `h2`, `h3`).\\nTo add element \"apple\":\\n*   `h1(\"apple\")` -> index 2\\n*   `h2(\"apple\")` -> index 5\\n*   `h3(\"apple\")` -> index 9\\nThe bits at positions 2, 5, and 9 in the bit array are set to `1`.\\n\\n#### 2. Checking for Membership\\n\\nTo check if an element `y` is in the set:\\n*   Apply each of the `k` hash functions to `y`.\\n*   Each hash function will produce an index.\\n*   Check the bits at all `k` computed indices in the bit array.\\n\\n**Two possible outcomes:**\\n\\n*   **If *any* of the `k` bits is `0`:** Then `y` is **definitely NOT** in the set. (Because if `y` had been added, all its corresponding bits would have been set to `1`).\\n*   **If *all* of the `k` bits are `1`:** Then `y` **might be** in the set. This is where the \"probabilistic\" nature comes in. There\\'s a chance that these `k` bits were all set to `1` by other elements that hashed to the same positions, even if `y` itself was never added. This is called a **false positive**.\\n\\n**Example (continuing from above):**\\nTo check for \"apple\":\\n*   `h1(\"apple\")` -> index 2 (bit is 1)\\n*   `h2(\"apple\")` -> index 5 (bit is 1)\\n*   `h3(\"apple\")` -> index 9 (bit is 1)\\nAll bits are `1`, so \"apple\" *might be* in the set. In this case, it *is* because we just added it.\\n\\nTo check for \"banana\" (which was never added):\\n*   `h1(\"banana\")` -> index 1 (bit is 0)\\n*   `h2(\"banana\")` -> index 5 (bit is 1)\\n*   `h3(\"banana\")` -> index 7 (bit is 0)\\nSince bits at index 1 and 7 are `0`, \"banana\" is **definitely NOT** in the set.\\n\\nTo check for \"grape\" (which was never added, but might cause a false positive):\\n*   `h1(\"grape\")` -> index 2 (bit is 1, set by \"apple\")\\n*   `h2(\"grape\")` -> index 5 (bit is 1, set by \"apple\")\\n*   `h3(\"grape\")` -> index 9 (bit is 1, set by \"apple\")\\nAll bits are `1`, so \"grape\" *might be* in the set, even though it wasn\\'t added. This is a false positive.\\n\\n### Key Properties\\n\\n*   **False Positives are Possible:** This is the main characteristic. The probability of a false positive increases as more elements are added to the filter, or if the filter is too small (`m` is too small) for the number of elements (`n`).\\n*   **False Negatives are NOT Possible:** If an element was added, its corresponding bits *must* be set to `1`. Therefore, a check will always correctly indicate that it *might* be in the set.\\n*   **Space Efficient:** Bloom filters use significantly less memory than storing the actual elements, especially for large sets.\\n*   **Cannot Delete Elements:** Once a bit is set to `1`, you cannot simply set it back to `0` when an element is removed. Doing so might inadvertently remove a bit that was also set by *another* element, leading to false negatives for that other element. (Variants like Counting Bloom Filters exist to address this, but they use more memory).\\n*   **Fixed Size:** Bloom filters are designed for a fixed number of elements. If you add too many, the false positive rate will become unacceptably high. Resizing requires rebuilding the entire filter.\\n*   **Fast Operations:** Adding and checking elements are very fast, as they only involve a few hash computations and bit manipulations, which are `O(k)` operations.\\n\\n### Parameters and Trade-offs\\n\\nThe performance of a Bloom filter depends on three key parameters:\\n\\n*   `m`: The size of the bit array (number of bits).\\n*   `k`: The number of hash functions.\\n*   `n`: The expected number of elements to be stored.\\n\\nThese parameters directly influence the false positive rate (`p`).\\n*   **Larger `m` (more bits):** Lower false positive rate, but more memory.\\n*   **Optimal `k` (number of hash functions):** There\\'s an optimal `k` for a given `m` and `n` that minimizes the false positive rate. Too few `k` means more collisions. Too many `k` means the bit array fills up too quickly, increasing collisions.\\n*   **Increasing `n` (more elements):** Increases the false positive rate for a fixed `m` and `k`.\\n\\n### Advantages\\n\\n*   **Extremely Space Efficient:** Its primary benefit, especially for huge datasets.\\n*   **Fast Operations:** Both insertion and lookup are constant time operations, dependent only on `k`.\\n*   **Privacy:** It doesn\\'t store the actual data, only a \"fingerprint\" of hashed bits, which can be beneficial in certain privacy-sensitive applications.\\n\\n### Disadvantages\\n\\n*   **False Positives:** The main drawback. You must be able to tolerate a certain error rate.\\n*   **Cannot Delete Elements (easily):** Makes it unsuitable for dynamic sets where elements are frequently removed.\\n*   **Fixed Capacity:** Requires an estimate of the maximum number of elements in advance.\\n\\n### Common Use Cases\\n\\nBloom filters are used in many areas where space and speed are critical, and a small error rate is acceptable:\\n\\n*   **Database Systems:** To avoid expensive disk lookups. For example, Cassandra and Google BigTable use Bloom filters to quickly check if a row exists in an SSTable before performing a costly disk read.\\n*   **Web Browsers/Caches:** To check if a URL has been visited recently (e.g., to avoid re-fetching content or to filter malicious URLs).\\n*   **Network Routers:** To filter out known malicious IP addresses or to identify unique packet flows.\\n*   **Spell Checkers:** To quickly check if a word is in a dictionary before performing a more complex lookup.\\n*   **Distributed Systems:** To synchronize data or identify unique items across multiple nodes without transferring large amounts of data.\\n*   **Password Breach Detection:** To check if a password has appeared in a breach database without sending the actual password to a server.\\n\\nIn summary, a Bloom filter is a powerful tool when you need to quickly check for set membership in a memory-constrained environment, as long as you can accept the occasional false positive.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--4b069969-7461-465d-8d1c-4aab736b24ed-0', usage_metadata={'input_tokens': 100, 'output_tokens': 3382, 'total_tokens': 3482, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1434}})]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Explain about Bloom filter in detail.\"}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1480439d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "A Bloom filter is a **space-efficient probabilistic data structure** that is used to test whether an element is a member of a set. It can tell you with certainty that an element is *not* in the set, or that it *might* be in the set (with a certain probability of error).\n",
      "\n",
      "Here's a detailed breakdown:\n",
      "\n",
      "### 1. What is a Bloom Filter?\n",
      "\n",
      "At its core, a Bloom filter is a simple array of bits (a **bitmap**), all initialized to 0. It uses multiple hash functions to add and check for the presence of elements.\n",
      "\n",
      "*   **Probabilistic:** This is its defining characteristic. While it never produces \"false negatives\" (it will never say an element is *not* in the set if it actually is), it *can* produce \"false positives\" (it might say an element *is* in the set when it actually isn't).\n",
      "*   **Space-Efficient:** It uses significantly less memory than storing the actual elements of the set.\n",
      "*   **Time-Efficient:** Adding and checking for elements are very fast, constant-time operations (O(k), where k is the number of hash functions).\n",
      "\n",
      "### 2. How it Works\n",
      "\n",
      "Let's break down the two main operations: adding an element and checking for an element.\n",
      "\n",
      "**Components:**\n",
      "\n",
      "*   A **bit array (bitmap)** of `m` bits, all initialized to 0.\n",
      "*   `k` independent **hash functions**, each designed to map an element to a position within the `m`-bit array.\n",
      "\n",
      "**a. Adding an Element (`x`) to the Filter:**\n",
      "\n",
      "1.  When you want to add an element `x` to the set represented by the Bloom filter, you first apply each of the `k` hash functions to `x`.\n",
      "2.  Each hash function `h_i(x)` will produce an index (a number between 0 and `m-1`).\n",
      "3.  For each of these `k` indices, you set the corresponding bit in the bit array to 1.\n",
      "    *   **Example:** If `h1(x)` points to bit 3, `h2(x)` points to bit 7, and `h3(x)` points to bit 12, then bits 3, 7, and 12 in the array are set to 1.\n",
      "    *   If a bit is already 1, it simply remains 1.\n",
      "\n",
      "**b. Checking for Membership (Querying) of an Element (`y`):**\n",
      "\n",
      "1.  To check if an element `y` is present in the set, you apply the *same* `k` hash functions to `y`.\n",
      "2.  Each hash function `h_i(y)` will again produce an index.\n",
      "3.  You then check the bits at all `k` of these calculated indices in the bit array:\n",
      "    *   **If *any* of the `k` bits is 0:** This means that `y` was *definitely not* added to the filter. (If it had been added, all its corresponding bits would have been set to 1). This is your **guaranteed \"false negative not possible\"** scenario.\n",
      "    *   **If *all* `k` bits are 1:** This means that `y` *might* be in the set. It's a \"possible member.\"\n",
      "        *   **False Positive Scenario:** It's possible that `y` was *not* added, but other elements that *were* added happened to set all the same `k` bits that `y` would hash to. This is where false positives come from.\n",
      "\n",
      "### 3. Key Characteristics and Properties\n",
      "\n",
      "*   **False Positives:** The probability of a false positive increases with the number of elements added to the filter and decreases with the size of the bit array (`m`) and the number of hash functions (`k`).\n",
      "*   **No False Negatives:** This is a strong guarantee. If the Bloom filter says an element is not present, it is 100% correct.\n",
      "*   **Space Efficiency:** It doesn't store the actual data, just a compact bit representation. This makes it ideal for very large datasets where memory is a concern.\n",
      "*   **Time Efficiency:** Insertion and lookup operations are constant time (O(k)), making them extremely fast regardless of the number of elements stored.\n",
      "*   **Cannot Delete Elements (Standard Bloom Filter):** Once a bit is set to 1, unsetting it would risk invalidating the membership checks for other elements that also hashed to that bit. (There are variations like \"counting Bloom filters\" that allow deletions, but they use more space).\n",
      "*   **Saturation:** As more elements are added, more bits become 1. Eventually, the filter can become \"saturated\" (most bits are 1), leading to a higher false positive rate.\n",
      "\n",
      "### 4. Parameters and Tuning\n",
      "\n",
      "The effectiveness of a Bloom filter depends on choosing the right values for:\n",
      "\n",
      "*   **`m` (size of the bit array):** A larger `m` reduces the false positive rate but uses more memory.\n",
      "*   **`k` (number of hash functions):** An optimal `k` exists for a given `m` and expected number of elements `n`. Too few hash functions lead to more collisions and higher false positives. Too many hash functions mean more bits are set per insertion, also increasing false positives and slowing down operations.\n",
      "*   **`n` (expected number of elements):** This is crucial for designing the filter.\n",
      "*   **`p` (desired false positive probability):** You typically start by deciding an acceptable false positive rate (e.g., 0.1% or 1%).\n",
      "\n",
      "There are formulas to calculate the optimal `m` and `k` given `n` and `p`.\n",
      "\n",
      "### 5. Advantages\n",
      "\n",
      "*   **Low memory footprint:** Ideal for large datasets.\n",
      "*   **Fast operations:** O(k) for insertion and lookup.\n",
      "*   **Guaranteed no false negatives:** If it says \"no,\" it's definitely no.\n",
      "\n",
      "### 6. Disadvantages\n",
      "\n",
      "*   **False positives:** The main drawback. You must be able to tolerate a certain error rate.\n",
      "*   **Cannot delete elements:** Standard Bloom filters don't support removal.\n",
      "*   **Fixed size:** Once created, its size (`m`) is fixed. If `n` significantly exceeds the expected number, the false positive rate will climb rapidly.\n",
      "\n",
      "### 7. Common Use Cases and Applications\n",
      "\n",
      "Bloom filters are incredibly versatile due to their speed and space efficiency, especially when dealing with large datasets where occasional false positives are acceptable:\n",
      "\n",
      "*   **Database Systems (like in `r2 ⋉ r1` context):**\n",
      "    *   **Join Optimization:** As mentioned in the prompt, for operations like `r2 ⋉ r1` (semi-join), a Bloom filter can be built from `r1` and sent to the node holding `r2`. `r2` can then filter out rows that *definitely don't* have a match in `r1` before sending them over the network for the actual join, saving bandwidth and processing.\n",
      "    *   **Checking for existence before costly operations:** Before performing an expensive disk read to check if a record exists, a Bloom filter can quickly determine if it's definitely not there.\n",
      "*   **Caching:** Checking if an item is *not* in a cache before making a slower lookup to the main storage.\n",
      "*   **Web Browsers:** Detecting malicious URLs. Google Chrome uses Bloom filters to quickly check if a URL is on a list of known bad sites before navigating to it.\n",
      "*   **Network Routers:** Packet filtering to quickly check if an IP address or port is allowed/blocked.\n",
      "*   **Spell Checkers:** Quickly identifying non-words to flag for correction.\n",
      "*   **Distributed Systems:** Detecting duplicate data across multiple servers without having to synchronize or compare full datasets.\n",
      "*   **URL Shorteners:** Checking if a newly generated short URL has already been used.\n",
      "\n",
      "In summary, a Bloom filter is a powerful tool for membership testing when you need high performance and space efficiency, and you can tolerate a small, configurable probability of false positives.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Explain about Bloom filter in detail.\"}]})\n",
    "print(result[\"messages\"][-1].pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bf4c6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Explain about B+ trees in detail.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "A B+-tree is a self-balancing tree data structure widely used in database management systems and file systems for efficient storage and retrieval of data. It's a variation of the B-tree, optimized particularly for disk-based storage and operations like range queries.\n",
      "\n",
      "Here's a detailed breakdown of B+-trees:\n",
      "\n",
      "### Core Structure and Key Differences from B-trees\n",
      "\n",
      "The fundamental difference between a B+-tree and a B-tree lies in how data is stored and retrieved, particularly in their non-leaf (internal) nodes versus leaf nodes.\n",
      "\n",
      "1.  **Non-Leaf (Internal) Nodes:**\n",
      "    *   These nodes store **only keys** and pointers to child nodes. They act purely as an index to guide the search down to the appropriate leaf node.\n",
      "    *   The provided context mentions what might \"appear in a nonleaf B-tree node, compared to B+-trees,\" implying that B-trees might store data pointers directly in internal nodes, whereas B+-trees strictly do not. This design choice in B+-trees allows more keys to fit into a single node (and thus a single disk block), reducing the number of disk I/O operations needed to traverse the tree.\n",
      "\n",
      "2.  **Leaf Nodes:**\n",
      "    *   All data records (or pointers to data records) are stored exclusively in the leaf nodes. Each leaf node contains the keys associated with its data.\n",
      "    *   Crucially, all leaf nodes are linked together in a **sequential manner** (typically a doubly linked list). This sequential linking is a defining feature and is vital for efficient range queries.\n",
      "\n",
      "3.  **Balanced Structure:**\n",
      "    *   Like B-trees, B+-trees are balanced, meaning all paths from the root to any leaf node have the same length. This ensures consistent search performance.\n",
      "\n",
      "4.  **Root Node:**\n",
      "    *   As stated in the provided text, \"the root has between 2 and n children,\" which is a standard property for B-tree variants, ensuring the tree doesn't degenerate and maintains a certain minimum occupancy.\n",
      "\n",
      "### How B+-trees Work\n",
      "\n",
      "1.  **Searching (Point Lookups):**\n",
      "    *   To find a specific record (a \"point lookup\"), the search starts at the root. It traverses internal nodes based on key comparisons, following the appropriate pointers, until it reaches the relevant leaf node. The full key and its associated data (or data pointer) are then retrieved from the leaf node. The provided text, though in the context of Bloom filters, highlights that B+-trees are very good for \"point lookups.\"\n",
      "\n",
      "2.  **Range Queries:**\n",
      "    *   This is where B+-trees truly excel. Once the first record within a desired range is found in a leaf node (using a standard search), the system can simply traverse the linked list of leaf nodes sequentially to retrieve all subsequent records within that range, without having to go back up the tree or perform multiple searches. This makes range queries extremely efficient.\n",
      "\n",
      "### Advantages of B+-trees\n",
      "\n",
      "*   **Efficient Range Queries:** The sequential linking of leaf nodes makes traversing a range of data extremely fast, as only one initial search is needed, followed by sequential reads.\n",
      "*   **Consistent Performance:** Due to the balanced structure, all searches (both point and range) take approximately the same amount of time, as all paths from the root to any leaf node are of equal length.\n",
      "*   **Optimized for Disk I/O:** By storing only keys in internal nodes, more keys can fit into a single disk block. This reduces the number of disk reads required to navigate the tree, as data is only loaded when a leaf node is accessed. This is a critical optimization for disk-based databases.\n",
      "*   **Good for Point Lookups:** They are efficient for finding individual records, as noted in the context.\n",
      "\n",
      "### Disadvantages of B+-trees\n",
      "\n",
      "*   **Insertion and Deletion Overhead:** The provided text explicitly states, \"the B+-tree structure imposes performance overhead on insertion and deletion.\" This is because maintaining the balanced structure, ensuring all keys are replicated in leaf nodes, and updating the sequential links between leaf nodes can involve complex operations like splitting or merging nodes, which can be computationally expensive.\n",
      "*   **Key Duplication:** Keys are duplicated in internal nodes (for indexing) and in leaf nodes (along with the data). While this is a design choice that facilitates efficiency, it does mean storing keys multiple times.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "In essence, B+-trees are highly effective indexing structures, particularly well-suited for scenarios in databases and file systems that require frequent range queries and efficient disk access. Despite the inherent overhead in modifying the tree structure (insertions and deletions), their strengths in retrieval performance make them a cornerstone of modern data management.\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain about B+ trees in detail.\"\n",
    "\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "014245ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store (myvectorstore) is ready: True\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# We use 'myvectorstore' created in Section 3.\n",
    "print(f\"Vector store (myvectorstore) is ready: {myvectorstore is not None}\")\n",
    "\n",
    "@dynamic_prompt\n",
    "def prompt_with_context_and_rewrite(request: ModelRequest) -> str:\n",
    "    # 1. Extract the user's last query from request.state[\"messages\"]\n",
    "    last_query = request.state[\"messages\"][-1].content\n",
    "    print(f\"\\n--- [Middleware] Original Query: '{last_query}' ---\")\n",
    "\n",
    "    # 2. Request to rewrite the query\n",
    "    rewrite_system_msg = \"\"\"You are an expert query assistant. Your task is to rewrite the user's question into an optimized query for a vector database search. Your rewritten query will be used for similarity search.\n",
    "    Only output the rewritten query.\"\"\"\n",
    "\n",
    "    # Make a template\n",
    "    rewrite_template = ChatPromptTemplate(\n",
    "        [\n",
    "            (\"system\", rewrite_system_msg),\n",
    "            (\"human\", \"{user_input}\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Fill in the template with the query content\n",
    "    rewrite_prompt_value = rewrite_template.invoke(\n",
    "        {\n",
    "            \"user_input\": last_query,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    rewrite_response = model.invoke(rewrite_prompt_value.messages)\n",
    "\n",
    "    rewritten_query = rewrite_response.content\n",
    "    print(f\"--- [Middleware] Rewritten Query: '{rewritten_query}' ---\")\n",
    "\n",
    "    # 3. Search for documents\n",
    "    try:\n",
    "      retrieved_docs = myvectorstore.similarity_search(rewritten_query, k=3) # Get top 3\n",
    "    except Exception as e:\n",
    "      print(f\"Check your vector store: {e}\")\n",
    "      retrieved_docs = []\n",
    "\n",
    "    # 4. Join the page_content of the retrieved docs into a single string\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    print(f\"--- [Middleware] Retrieved {len(retrieved_docs)} docs ---\")\n",
    "\n",
    "    # 5. Dynamically create the system prompt to be sent to the LLM\n",
    "    system_message = (\n",
    "        \"You are a helpful assistant. Use the following context in your response:\"\n",
    "        \"\\n\\n--- CONTEXT ---\"\n",
    "        f\"\\n{docs_content}\"\n",
    "        \"\\n--- END CONTEXT ---\"\n",
    "    )\n",
    "\n",
    "    return system_message\n",
    "\n",
    "agent = create_agent(model, tools=[], middleware=[prompt_with_context_and_rewrite])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "daed052b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I'm working on my homework. Please explain me about the transaction. There are database system concepts in the vector database.\n",
      "\n",
      "--- [Middleware] Original Query: 'I'm working on my homework. Please explain me about the transaction. There are database system concepts in the vector database.' ---\n",
      "--- [Middleware] Rewritten Query: 'Explain transactions, ACID properties, concurrency control, and recovery mechanisms within the context of database system concepts, specifically detailing their relevance and implementation in vector databases.' ---\n",
      "--- [Middleware] Retrieved 3 docs ---\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Based on the provided text, here's what we can gather about transactions in database systems:\n",
      "\n",
      "*   **Core Properties:** Transactions are central to implementing the **atomicity** and **durability** properties within a database.\n",
      "    *   **Atomicity:** This means that a transaction is treated as a single, indivisible unit of work. Either all of its operations are completed successfully, or none of them are.\n",
      "    *   **Durability:** This ensures that once a transaction has successfully completed and committed, its changes are permanently stored and will survive any subsequent system failures.\n",
      "*   **Recovery Management:** The text mentions that the **recovery management component** of a database is responsible for implementing these atomicity and durability properties, which are fundamental to how transactions behave.\n",
      "*   **Transaction Processing:** The text also refers to \"Transaction processing,\" indicating that transactions are units of work that are executed and managed within the database system.\n",
      "*   **General Concept:** Transactions are a fundamental \"database system concept,\" as implied by their discussion in the context of general database system features and components.\n",
      "\n",
      "Regarding **vector databases**, the provided text does not contain any specific information about transactions within vector database systems. The context focuses on general database system concepts and properties like atomicity and durability.\n"
     ]
    }
   ],
   "source": [
    "query = \"I'm working on my homework. Please explain me about the transaction. There are database system concepts in the vector database.\"\n",
    "\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd886f",
   "metadata": {},
   "source": [
    "# Evaluate the RAG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e597c3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"YOUR API KEY\" # enter your langsmith api key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25728fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Context Holder Ready ---\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# (1) Define a simple helper class\n",
    "class RAGContextHolder:\n",
    "    def __init__(self):\n",
    "        # A variable to store the most recently retrieved docs\n",
    "        self.last_retrieved_docs = []\n",
    "\n",
    "    def set_docs(self, docs: list[Document]):\n",
    "        \"\"\"Called by the middleware to save the retrieved docs\"\"\"\n",
    "        self.last_retrieved_docs = docs\n",
    "\n",
    "    def get_docs(self) -> list[Document]:\n",
    "        \"\"\"Called by the evaluation function to get the saved docs\"\"\"\n",
    "        return self.last_retrieved_docs\n",
    "\n",
    "# (2) Create a \"global\" instance of this class\n",
    "context_holder = RAGContextHolder()\n",
    "\n",
    "print(\"--- Context Holder Ready ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f368ae54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store (myvectorstore) is ready: True\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "print(f\"Vector store (myvectorstore) is ready: {myvectorstore is not None}\")\n",
    "\n",
    "@dynamic_prompt\n",
    "def prompt_with_context_and_rewrite(request: ModelRequest) -> str:\n",
    "    last_query = request.state[\"messages\"][-1].content\n",
    "    print(f\"\\n--- [Middleware] Original Query: '{last_query}' ---\")\n",
    "\n",
    "    rewrite_system_msg = \"\"\"You are an expert query assistant. Your task is to rewrite the user's question into an optimized query for a vector database search. Your rewritten query will be used for similarity search.\n",
    "    Only output the rewritten query.\"\"\"\n",
    "\n",
    "    rewrite_template = ChatPromptTemplate(\n",
    "        [\n",
    "            (\"system\", rewrite_system_msg),\n",
    "            (\"human\", \"{user_input}\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    rewrite_prompt_value = rewrite_template.invoke(\n",
    "        {\n",
    "            \"user_input\": last_query,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    rewrite_response = model.invoke(rewrite_prompt_value.messages)\n",
    "\n",
    "    rewritten_query = rewrite_response.content\n",
    "    print(f\"--- [Middleware] Rewritten Query: '{rewritten_query}' ---\")\n",
    "\n",
    "    try:\n",
    "      retrieved_docs = myvectorstore.similarity_search(rewritten_query, k=3) # Get top 3\n",
    "    except Exception as e:\n",
    "      print(f\"Check your vector store: {e}\")\n",
    "      retrieved_docs = []\n",
    "\n",
    "    ############### NEW STEP - store the retrieved docs ##################\n",
    "    context_holder.set_docs(retrieved_docs)\n",
    "    print(f\"--- [Middleware] Saved {len(retrieved_docs)} docs to Context Holder ---\")\n",
    "    ######################################################################\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    print(f\"--- [Middleware] Retrieved {len(retrieved_docs)} docs ---\")\n",
    "\n",
    "    system_message = (\n",
    "        \"You are a helpful assistant. Use the following context in your response:\"\n",
    "        \"\\n\\n--- CONTEXT ---\"\n",
    "        f\"\\n{docs_content}\"\n",
    "        \"\\n--- END CONTEXT ---\"\n",
    "    )\n",
    "\n",
    "    return system_message\n",
    "\n",
    "agent = create_agent(model, tools=[], middleware=[prompt_with_context_and_rewrite])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83aad855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Wrapper Function Test ---\n",
      "\n",
      "--- [Middleware] Original Query: 'Explain about Bloom filter in detail.' ---\n",
      "--- [Middleware] Rewritten Query: 'Detailed explanation of Bloom filter' ---\n",
      "--- [Middleware] Saved 3 docs to Context Holder ---\n",
      "--- [Middleware] Retrieved 3 docs ---\n",
      "Answer: Based on the provided context, here's what we know...\n",
      "Documents Count: 3\n"
     ]
    }
   ],
   "source": [
    "def run_agent_for_evaluation(input_query: str) -> dict:\n",
    "    \"\"\"\n",
    "    A wrapper function that LangSmith evaluation will call.\n",
    "    inputs_dict must be in the format {\"question\": \"...\"}.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Run the agent\n",
    "    # (This call internally triggers the 'prompt_with_context_and_rewrite_and_save' middleware)\n",
    "    result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": input_query}]})\n",
    "    answer = result[\"messages\"][-1].content\n",
    "\n",
    "    # 2. Get the \"hidden\" retrieved docs\n",
    "    retrieved_docs = context_holder.get_docs()\n",
    "\n",
    "    # 3. Return in the format required by the evaluation tutorial\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"documents\": [d.page_content for d in retrieved_docs]\n",
    "    }\n",
    "\n",
    "# test\n",
    "print(\"--- Wrapper Function Test ---\")\n",
    "test_output = run_agent_for_evaluation(\"Explain about Bloom filter in detail.\")\n",
    "print(f\"Answer: {test_output['answer'][:50]}...\")\n",
    "print(f\"Documents Count: {len(test_output['documents'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68de77f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target(inputs: dict) -> dict:\n",
    "    return run_agent_for_evaluation(inputs[\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bcd244c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Annotated, TypedDict\n",
    "from langchain.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# output schema for structured output\n",
    "class RelevanceGrade(TypedDict):\n",
    "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
    "    relevant: Annotated[\n",
    "        int, ..., \"Score from 1 to 5, where 5 is most relevant and 1 is least relevant\"\n",
    "    ]\n",
    "\n",
    "# Grade prompt\n",
    "relevance_instructions = \"\"\"You are an impartial evaluator. Your task is to assess the relevance of a provided ANSWER to a given QUESTION using a 1-5 score.\n",
    "\n",
    "You will be given a QUESTION and an ANSWER. Here is the grading criteria:\n",
    "- **1 (Poor):** The ANSWER is completely off-topic, evasive, or does not address the QUESTION at all.\n",
    "- **2 (Fair):** The ANSWER is tangentially related but does not directly answer the core of the QUESTION.\n",
    "- **3 (Average):** The ANSWER partially addresses the QUESTION but misses key aspects or includes irrelevant information.\n",
    "- **4 (Good):** The ANSWER directly addresses the QUESTION and is helpful, but could be slightly more complete or concise.\n",
    "- **5 (Excellent):** The ANSWER directly, fully, and helpfully addresses the QUESTION's intent.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner. First, analyze the question's intent. Second, analyze the answer's content. Finally, provide your score from 1 to 5.\n",
    "\"\"\"\n",
    "\n",
    "# Grader LLM\n",
    "relevance_llm = model.with_structured_output(\n",
    "    RelevanceGrade, method=\"json_schema\", strict=True\n",
    ")\n",
    "\n",
    "# Evaluator\n",
    "def relevance(inputs: dict, outputs: dict) -> bool:\n",
    "    messages = [\n",
    "        SystemMessage(content=relevance_instructions),\n",
    "        HumanMessage(content=f\"QUESTION: {inputs['query']}\\nANSWER: {outputs['answer']}\")\n",
    "    ]\n",
    "    grade = relevance_llm.invoke(messages)\n",
    "    return grade[\"relevant\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e52f9c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade output schema\n",
    "class GroundedGrade(TypedDict):\n",
    "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
    "    grounded: Annotated[\n",
    "        int, ..., \"Score from 1 to 5, where 5 is fully grounded and 1 is hallucinated\"\n",
    "    ]\n",
    "\n",
    "# Grade prompt\n",
    "grounded_instructions = \"\"\"You are an impartial evaluator. Your task is to assess whether an ANSWER is \"grounded in\" a set of provided CONTEXTS using a 1-5 score.\n",
    "\n",
    "You will be given a set of CONTEXTS and an ANSWER. Here are the grading criteria:\n",
    "- **1 (Not Grounded):** The ANSWER contains significant information or claims that are NOT supported by the CONTEXTS (i.e., hallucination).\n",
    "- **2 (Poorly Grounded):** The ANSWER contains some claims that are not supported, or significantly misrepresents the CONTEXTS.\n",
    "- **3 (Partially Grounded):** The ANSWER is mostly supported by the CONTEXTS, but may contain minor claims or details not found in the CONTEXTS.\n",
    "- **4 (Well Grounded):** The ANSWER is almost entirely supported by the CONTEXTS, with only very minor embellishments.\n",
    "- **5 (Fully Grounded):** Every single claim in the ANSWER is explicitly supported by the provided CONTEXTS.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner. First, break down the ANSWER into individual claims. Second, for each claim, check if it is supported by the CONTEXTS. Finally, provide your score from 1 to 5.\n",
    "\"\"\"\n",
    "\n",
    "# Grader LLM\n",
    "grounded_llm = model.with_structured_output(\n",
    "    GroundedGrade, method=\"json_schema\", strict=True\n",
    ")\n",
    "\n",
    "# Evaluator\n",
    "def groundedness(inputs: dict, outputs: dict) -> bool:\n",
    "# --- FIX ---\n",
    "    # The 'run_agent_for_evaluation' wrapper returns a list of strings in the 'documents' key\n",
    "    if not outputs[\"documents\"]:\n",
    "        # If no document was retrieved, any answer (other than \"I don't know\") is by definition ungrounded.\n",
    "        return 1\n",
    "\n",
    "    doc_string = \"\\n\\n\".join(outputs[\"documents\"])\n",
    "\n",
    "    answer_string = f\"CONTEXTS: {doc_string}\\n\\nANSWER: {outputs['answer']}\"\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=grounded_instructions),\n",
    "        HumanMessage(content=answer_string)\n",
    "    ]\n",
    "\n",
    "    grade = grounded_llm.invoke(messages)\n",
    "    return grade[\"grounded\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a007da94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade output schema\n",
    "class RetrievalRelevanceGrade(TypedDict):\n",
    "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
    "    retrieval: Annotated[\n",
    "        int,\n",
    "        ...,\n",
    "        \"Score from 1 to 5, where 5 is highly relevant and 1 is not relevant\",\n",
    "    ]\n",
    "\n",
    "# Grade prompt\n",
    "retrieval_relevance_instructions = \"\"\"You are an impartial evaluator. Your task is to assess the relevance of a set of retrieved CONTEXTS to a given QUESTION using a 1-5 score.\n",
    "\n",
    "You will be given a QUESTION and a set of CONTEXTS. Here are the grading criteria:\n",
    "- **1 (Poor):** ALL retrieved CONTEXTS are completely irrelevant to the QUESTION.\n",
    "- **2 (Fair):** Most CONTEXTS are irrelevant, but one or two might be tangentially related.\n",
    "- **3 (Average):** Some CONTEXTS are relevant to the QUESTION, but many are irrelevant or contain noise.\n",
    "- **4. (Good):** Most CONTEXTS are relevant and helpful for answering the QUESTION.\n",
    "- **5 (Excellent):** ALL retrieved CONTEXTS are highly relevant and crucial for answering the QUESTION.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner. First, analyze the QUESTION's intent. Second, examine each CONTEXT for its relevance. Finally, provide your score from 1 to 5 based on the overall relevance of the set.\n",
    "\"\"\"\n",
    "\n",
    "# Grader LLM\n",
    "retrieval_relevance_llm = model.with_structured_output(RetrievalRelevanceGrade, method=\"json_schema\", strict=True)\n",
    "\n",
    "def retrieval_relevance(inputs: dict, outputs: dict) -> bool:\n",
    "    \"\"\"An evaluator for document relevance\"\"\"\n",
    "\n",
    "    if not outputs[\"documents\"]:\n",
    "        return 1 # No contexts retrieved, so they cannot be relevant.\n",
    "\n",
    "    doc_string = \"\\n\\n\".join(outputs[\"documents\"])\n",
    "\n",
    "    answer_string = f\"CONTEXTS: {doc_string}\\n\\nQUESTION: {inputs['query']}\"\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=retrieval_relevance_instructions),\n",
    "        HumanMessage(content=answer_string)\n",
    "    ]\n",
    "\n",
    "    # Run evaluator\n",
    "    grade = retrieval_relevance_llm.invoke(messages)\n",
    "    return grade[\"retrieval\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9e4fb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"inputs\": {\"query\": \"Explain about Bloom filter in detail.\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"query\": \"Explain about B+ trees in detail.\"},\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ae24ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['e1aff425-b616-410a-8828-7deed2d1fa4b',\n",
       "  'f651a2f3-99d7-4b46-839f-f4ee486e525a'],\n",
       " 'count': 2}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"RAG evaluation_01\"\n",
    "dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "client.create_examples(\n",
    "    dataset_id=dataset.id,\n",
    "    examples=examples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c10d51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-doc-relevance-3d9bf098' at:\n",
      "https://smith.langchain.com/o/779968f0-7271-4507-b369-04e333f1ec3d/datasets/a8c4466f-90df-4c3c-8f5f-744be462bcfe/compare?selectedSessions=4be3fc6f-8bb2-4e52-831d-d3d2ffd22790\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [Middleware] Original Query: 'Explain about Bloom filter in detail.' ---\n",
      "--- [Middleware] Rewritten Query: 'Bloom filter: detailed explanation, working principles, use cases, advantages, disadvantages, and applications.' ---\n",
      "--- [Middleware] Saved 3 docs to Context Holder ---\n",
      "--- [Middleware] Retrieved 3 docs ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:24, 24.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [Middleware] Original Query: 'Explain about B+ trees in detail.' ---\n",
      "--- [Middleware] Rewritten Query: 'B+ trees detailed explanation' ---\n",
      "--- [Middleware] Saved 3 docs to Context Holder ---\n",
      "--- [Middleware] Retrieved 3 docs ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [01:10, 35.36s/it]\n"
     ]
    }
   ],
   "source": [
    "experiment_results = client.evaluate(\n",
    "    target,\n",
    "    data=dataset_name,\n",
    "    evaluators=[groundedness, relevance, retrieval_relevance],\n",
    "    experiment_prefix=\"rag-doc-relevance\",\n",
    "    metadata={\"version\": \"none\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6bff05c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.query</th>\n",
       "      <th>outputs.answer</th>\n",
       "      <th>outputs.documents</th>\n",
       "      <th>error</th>\n",
       "      <th>feedback.groundedness</th>\n",
       "      <th>feedback.relevance</th>\n",
       "      <th>feedback.retrieval_relevance</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explain about Bloom filter in detail.</td>\n",
       "      <td>Based on the provided context:\\n\\nA Bloom filt...</td>\n",
       "      <td>[Bloom ﬁlter, which uses bitmaps. Bloom ﬁlters...</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4.808055</td>\n",
       "      <td>e1aff425-b616-410a-8828-7deed2d1fa4b</td>\n",
       "      <td>019ad410-6f1f-7729-8fb0-9c5f17d7168d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Explain about B+ trees in detail.</td>\n",
       "      <td>Based on the provided context, here's what we ...</td>\n",
       "      <td>[appear in a nonleaf B-tree node, compared to ...</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9.219572</td>\n",
       "      <td>f651a2f3-99d7-4b46-839f-f4ee486e525a</td>\n",
       "      <td>019ad410-cf2c-7199-aecf-3ee9e907c4b3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            inputs.query  \\\n",
       "0  Explain about Bloom filter in detail.   \n",
       "1      Explain about B+ trees in detail.   \n",
       "\n",
       "                                      outputs.answer  \\\n",
       "0  Based on the provided context:\\n\\nA Bloom filt...   \n",
       "1  Based on the provided context, here's what we ...   \n",
       "\n",
       "                                   outputs.documents error  \\\n",
       "0  [Bloom ﬁlter, which uses bitmaps. Bloom ﬁlters...  None   \n",
       "1  [appear in a nonleaf B-tree node, compared to ...  None   \n",
       "\n",
       "   feedback.groundedness  feedback.relevance  feedback.retrieval_relevance  \\\n",
       "0                      5                   4                             2   \n",
       "1                      2                   5                             4   \n",
       "\n",
       "   execution_time                            example_id  \\\n",
       "0        4.808055  e1aff425-b616-410a-8828-7deed2d1fa4b   \n",
       "1        9.219572  f651a2f3-99d7-4b46-839f-f4ee486e525a   \n",
       "\n",
       "                                     id  \n",
       "0  019ad410-6f1f-7729-8fb0-9c5f17d7168d  \n",
       "1  019ad410-cf2c-7199-aecf-3ee9e907c4b3  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_results.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432265cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raptor-rag-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
